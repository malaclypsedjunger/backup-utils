#!/usr/bin/env bash
#/ Usage: ghe-restore-repositories-rsync <host>
#/ Restore an rsync snapshot of all Git repository data to a GitHub instance.
#/
#/ Note: This script typically isn't called directly. It's invoked by the
#/ ghe-restore command when the rsync strategy is used.
set -e

# Bring in the backup configuration
# shellcheck source=share/github-backup-utils/ghe-backup-config
. "$( dirname "${BASH_SOURCE[0]}" )/ghe-backup-config"

# Show usage and bail with no arguments
[ -z "$*" ] && print_usage

bm_start "$(basename $0)"

# Grab host arg
GHE_HOSTNAME="$1"

# Perform a host-check and establish GHE_REMOTE_XXX variables.
ghe_remote_version_required "$GHE_HOSTNAME"

# The snapshot to restore should be set by the ghe-restore command but this lets
# us run this script directly.
: ${GHE_RESTORE_SNAPSHOT:=current}

cleanup() {
  # Enable remote GC operations
  ghe-gc-enable $GHE_HOSTNAME
}

trap 'cleanup' INT TERM EXIT

# Disable remote GC operations
ghe-gc-disable $GHE_HOSTNAME

# Transfer all git repository data from the latest snapshot to the GitHub
# using multiple concurrent rsyncs.
# Break up the transfer by top level directory under repositories/
# and do each one in parallel (there should be 17 of them - with the 'info"
# directory being a very small directory).
# On a system with 16 or more cores, it probably pays to break up the list
# of files into more than 17 groups (even with fewer cores, a finer break up
# will help "even out" the completions times and achieve more effective
# parallelism but I would like buy in to this approach before doing the
# work to do that).
# Be sure to disable the sharing of a persistent ssh connection as that would
# defeat the purpose of doing things in parallel.
export GHE_DISABLE_SSH_MUX=1
for item in $GHE_DATA_DIR/$GHE_RESTORE_SNAPSHOT/repositories/*
do
	ghe-rsync -avH --delete \
	    --exclude ".sync_in_progress" \
	    -e "ghe-ssh -p $(ssh_port_part "$GHE_HOSTNAME")" \
	    --rsync-path="sudo -u git rsync" \
	    "$item" \
	    "$(ssh_host_part "$GHE_HOSTNAME"):$GHE_REMOTE_DATA_USER_DIR/repositories/" 1>&3 &
done

# wait for all of them to finish and check status
while "$(jobs -p)" -ne ""
do
	wait -n $(jobs -p)
	if [ $? -ne 0 ]
	then
		echo "oops $?"
		bm_end "$(basename $0)"
		exit 1
	else
		echo "done $?"
		exit 0
	fi
done

bm_end "$(basename $0)"

exit 0

